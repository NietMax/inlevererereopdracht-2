{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetisch algioritme voor optimale hyper-parameters verkrijgen uit de iris dataset\n",
    "\n",
    "Deze code-cel begint met het importeren van de benodigde Python-bibliotheken en instellingen voor het uitvoeren van hyperparameteroptimalisatie voor een neuraal netwerkmodel. Hier zijn de belangrijkste elementen die in deze cel gebeuren:\n",
    "\n",
    "1. **Importeren van Numpy en scikit-learn Libraries**:\n",
    "   - De code importeert `numpy` als `np` voor numerieke berekeningen.\n",
    "   - Het laadt de Iris-dataset met behulp van de `load_iris` functie van scikit-learn. Deze dataset zal worden gebruikt voor het evalueren van het neuraal netwerkmodel.\n",
    "   - Het importeert `train_test_split` om de dataset te splitsen in trainings- en testgegevens.\n",
    "   - Het gebruikt `MLPRegressor` van scikit-learn om een meerlaagse perceptron-neuraal netwerk voor regressie te definiëren en te configureren.\n",
    "\n",
    "2. **Importeren van de Tabulate-bibliotheek**:\n",
    "   - De code importeert de `tabulate`-bibliotheek. Deze bibliotheek wordt later gebruikt om de resulterende error metrics op een nette manier als een tabel weer te geven.\n",
    "\n",
    "3. **Importeren van Regressiemetrieken uit sklearn**\n",
    "   - De code importeert de `mean_absolute_error`, `mean_squared_error` en `r2_score` scoringsmetrieken om het genetisch regressie algoritme te scoren.\n",
    "\n",
    "4. **Uitschakelen van Waarschuwingen**:\n",
    "   - Er wordt een waarschuwingssuppressie geactiveerd om te voorkomen dat waarschuwingen de uitvoer onnodig vervuilen.\n",
    "\n",
    "Deze voorbereidende stappen zijn cruciaal voor het correct uitvoeren van de hyperparameteroptimalisatie voor het neurale netwerkmodel en voor het weergeven van de resultaten in een overzichtelijke tabel met error metrics aan het einde van het script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports for the hyperparameter optimization\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Import the tabulate library for a nice table at the end of the file for the error metrics\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Importeer relevante regressiemetrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Ignoring all warnings because it clutters the space where the results are supposed to go in to\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gebruik van de Iris Dataset en Dataverdeling\n",
    "\n",
    "In deze code-cel wordt de Iris-dataset gebruikt en wordt de dataset verdeeld in trainings- en testgegevens. Hier zijn de belangrijkste elementen die in deze cel gebeuren:\n",
    "\n",
    "1. **Laden van de Iris Dataset**:\n",
    "   - De code laadt de Iris-dataset met behulp van de `load_iris` functie. De Iris-dataset bevat informatie over irisbloemen en wordt vaak gebruikt voor machinaal leren toepassingen.\n",
    "\n",
    "2. **Dataverdeling**:\n",
    "   - De code splitst de gegevens in onafhankelijke variabelen (`X`) en de doelvariabele (`y`). `X` bevat de kenmerken van de bloemen, terwijl `y` de bijbehorende klasselabels bevat.\n",
    "   - De dataset wordt verder opgesplitst in trainings- en testgegevens met behulp van `train_test_split`. Hierbij wordt 80% van de gegevens toegewezen aan de trainingsset en 20% aan de testset.\n",
    "   - De `random_state` wordt ingesteld op 42 (“the Ultimate Question of Life, the Universe, and Everything.\") om ervoor te zorgen dat de verdeling van de gegevens reproduceerbaar is.\n",
    "\n",
    "Deze stappen zijn essentieel voor het voorbereiden van de gegevens voor de training en evaluatie van het neurale netwerkmodel. De Iris-dataset wordt gebruikt als de basis voor dit machine learning-assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're still using the iris dataset for this assignment\n",
    "data = load_iris()\n",
    "\n",
    "# Splitting the data into training and testing data\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitie van te Testen Hyperparameters\n",
    "\n",
    "In deze code-cel worden de hyperparameters gedefinieerd die zullen worden getest bij het optimaliseren van het neurale netwerkmodel. Hier zijn de belangrijkste elementen die in deze cel gebeuren:\n",
    "\n",
    "1. **Activatiefuncties**:\n",
    "   - De code definieert een lijst `activation_functions` met verschillende activatiefuncties die zullen worden getest. Deze functies beïnvloeden de overgang van informatie tussen neuronen in het netwerk. De beschikbare activatiefuncties zijn 'logistic', 'tanh', 'relu', en 'identity'.\n",
    "\n",
    "2. **Grootte van Verborgen Lagen**:\n",
    "   - De grootte van de verborgen lagen wordt gedefinieerd als een lijst van getallen in `hidden_layer_sizes`. Elke waarde in de lijst vertegenwoordigt de grootte van een verborgen laag in het neurale netwerk. Dit geeft flexibiliteit bij het testen van verschillende architecturen.\n",
    "\n",
    "3. **Optimalisatie Solvers**:\n",
    "   - De code definieert een lijst `solvers` met verschillende optimalisatiesolvers die worden gebruikt om de optimale gewichten voor het neurale netwerk te vinden. De beschikbare solvers zijn 'lbfgs', 'sgd', en 'adam'. Het standaard leerpercentage wordt gebruikt voor elk van deze solvers.\n",
    "\n",
    "Deze definitie van hyperparameters is cruciaal voor het begeleiden van de hyperparameteroptimalisatie van het neurale netwerkmodel. Door verschillende combinaties van activatiefuncties, verborgen laaggroottes en solvers te testen, kan de optimale configuratie van het model worden bepaald.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the hyperparameters that we want to test\n",
    "activation_functions = ['logistic', 'tanh', 'relu', 'identity']\n",
    "# The hidden layer sizes are defined as a list of tuples, where each tuple represents a hidden layer\n",
    "hidden_layer_sizes = [10, 20, 30, 40]\n",
    "# The solvers are the optimizers that are used to find the optimal weights\n",
    "# We're using the default learning rate for each solver\n",
    "solvers = ['lbfgs', 'sgd', 'adam']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uitvoeren van het Genetisch Algoritme voor Klassificatie en Afdrukken van Resultaten\n",
    "\n",
    "In deze code-cel wordt het genetisch algoritme voor klassificatie uitgevoerd om de optimale hyperparameters voor het neurale netwerkmodel te vinden, en worden de resultaten afgedrukt. Hier zijn de belangrijkste elementen die in deze cel gebeuren:\n",
    "\n",
    "**Uitvoeren van het Genetisch Algoritme**:\n",
    "- De functie `genetic_algorithm` wordt opgeroepen met twee parameters: `population_size` en `generations`. Hier worden respectievelijk de grootte van de populatie en het aantal generaties voor het genetisch algoritme gespecificeerd.\n",
    "- Het genetisch algoritme zal meerdere generaties doorlopen en proberen de optimale hyperparameters te vinden op basis van de prestaties van individuen. Dit omvat strategieën zoals \"crossover\" en \"mutation\".\n",
    "\n",
    "**Crossover en Mutation**:\n",
    "- In het genetisch algoritme wordt \"crossover\" toegepast, waarbij de hyperparameters van twee ouderindividuen worden gecombineerd om nieuwe kindindividuen te creëren. Dit kan bijdragen aan diversiteit en het ontdekken van betere hyperparametercombinaties.\n",
    "- Bovendien wordt \"mutation\" gebruikt, waarbij willekeurige wijzigingen worden aangebracht in de hyperparameters van individuen. Dit kan helpen bij het verkennen van nieuwe hyperparameterwaarden.\n",
    "\n",
    "**Afdrukken van de Optimale Hyperparameters**:\n",
    "- Nadat het genetisch algoritme is voltooid, worden de optimale hyperparameters verkregen en opgeslagen in de variabele `best_hyperparameters`.\n",
    "- Deze optimale hyperparameters worden afgedrukt naar de console met de tekst \"Optimale hyperparameters:\", gevolgd door de daadwerkelijke hyperparameters.\n",
    "\n",
    "Het doel van deze code is om de beste hyperparameters te vinden voor een neurale netwerkmodel voor klassificatie door middel van een genetisch algoritme, waarbij strategieën zoals \"crossover\" en \"mutation\" worden toegepast. Deze hyperparameters zijn essentieel voor het verbeteren van de prestaties van het model bij het oplossen van klassificatieproblemen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function that evaluates the accuracy of an individual\n",
    "def evaluate_individual(activation_function, hidden_layer_size, solver):\n",
    "    reg = MLPRegressor(hidden_layer_sizes=(hidden_layer_size,), activation=activation_function, solver=solver, max_iter=2000, random_state=42)\n",
    "    reg.fit(X_train, y_train)\n",
    "    accuracy = reg.score(X_test, y_test)\n",
    "    return accuracy\n",
    "\n",
    "# The genetic algorithm that finds the best individual with the given hyperparameters\n",
    "def genetic_algorithm(population_size, generations):\n",
    "    # Defining the mutation rate and the initial population\n",
    "    mutation_rate = 0.1\n",
    "    population = []\n",
    "\n",
    "    # Creating the initial population\n",
    "    for _ in range(population_size):\n",
    "        activation_function = np.random.choice(activation_functions)\n",
    "        hidden_layer_size = np.random.choice(hidden_layer_sizes)\n",
    "        solver = np.random.choice(solvers)\n",
    "        individual = (activation_function, hidden_layer_size, solver)\n",
    "        population.append(individual)\n",
    "\n",
    "    # The genetic algorithm\n",
    "    for generation in range(generations):\n",
    "        scores = [evaluate_individual(activation_function, hidden_layer_size, solver) for activation_function, hidden_layer_size, solver in population]\n",
    "        selected_indices = np.argsort(scores)[::-1][:int(population_size * 0.2)]\n",
    "        selected_population = [population[i] for i in selected_indices]\n",
    "        new_population = []\n",
    "        \n",
    "        # Creating the new population with random crossover and mutation\n",
    "        for _ in range(population_size):\n",
    "            index1, index2 = np.random.choice(len(selected_population), size=2, replace=False)\n",
    "            parent1 = selected_population[index1]\n",
    "            parent2 = selected_population[index2]\n",
    "\n",
    "            if np.random.rand() < 0.5:\n",
    "                child = (parent1[0], parent2[1], parent2[2])\n",
    "            else:\n",
    "                child = (parent2[0], parent1[1], parent1[2])\n",
    "\n",
    "            new_population.append(child)\n",
    "\n",
    "        # Mutating the new population\n",
    "        for i in range(population_size):\n",
    "            if np.random.rand() < mutation_rate:\n",
    "                mutation_type = np.random.choice(['activation', 'hidden_layer', 'solver'])\n",
    "                if mutation_type == 'activation':\n",
    "                    new_population[i] = (np.random.choice(activation_functions), new_population[i][1], new_population[i][2])\n",
    "                elif mutation_type == 'hidden_layer':\n",
    "                    new_population[i] = (new_population[i][0], np.random.choice(hidden_layer_sizes), new_population[i][2])\n",
    "                elif mutation_type == 'solver':\n",
    "                    new_population[i] = (new_population[i][0], new_population[i][1], np.random.choice(solvers))\n",
    "    \n",
    "        # Setting the new population as the current population\n",
    "        population = new_population\n",
    "\n",
    "    # Returning the best individual\n",
    "    best_individual = max(population, key=lambda ind: evaluate_individual(ind[0], ind[1], ind[2]))\n",
    "    return best_individual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uitvoeren van het Genetisch Algoritme en Afdrukken van Resultaten\n",
    "\n",
    "In deze code-cel wordt het genetisch algoritme uitgevoerd om de optimale hyperparameters voor het neurale netwerkmodel te vinden, en worden de resultaten afgedrukt. Hier zijn de belangrijkste elementen die in deze cel gebeuren:\n",
    "\n",
    "**Uitvoeren van het Genetisch Algoritme**:\n",
    "- De functie `genetic_algorithm` wordt opgeroepen met twee parameters: `population_size` en `generations`. Hier worden respectievelijk de grootte van de populatie en het aantal generaties voor het genetisch algoritme gespecificeerd.\n",
    "- Het genetisch algoritme zal meerdere generaties doorlopen en proberen de optimale hyperparameters te vinden op basis van de prestaties van individuen.\n",
    "\n",
    "**Afdrukken van de Optimale Hyperparameters**:\n",
    "- Nadat het genetisch algoritme is voltooid, worden de optimale hyperparameters verkregen en opgeslagen in de variabele `best_hyperparameters`.\n",
    "- Deze optimale hyperparameters worden afgedrukt naar de console met de tekst \"Optimal hyperparameters:\", gevolgd door de daadwerkelijke hyperparameters.\n",
    "\n",
    "Deze code is de laatste stap in het proces van hyperparameteroptimalisatie en onthult de optimale configuratie die is gevonden door het genetisch algoritme. Dit is belangrijke informatie om de prestaties van het neurale netwerkmodel te verbeteren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal hyperparameters: ('tanh', 10, 'lbfgs')\n"
     ]
    }
   ],
   "source": [
    "# Running the genetic algorithm and printing the results\n",
    "best_hyperparameters = genetic_algorithm(population_size=50, generations=10)\n",
    "print(\"Optimal hyperparameters:\", best_hyperparameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uitvoeren van het Genetisch Algoritme voor Regressie en de Evaluatie van het Geoptimaliseerde Neurale Netwerk Model Algoritme (met de juiste error metrics)\n",
    "\n",
    "In deze code-cel wordt het genetisch algoritme voor regressie uitgevoerd om de optimale hyperparameters voor het neurale netwerkmodel te vinden, en worden de resultaten afgedrukt. Hier zijn de belangrijkste elementen die in deze cel gebeuren:\n",
    "\n",
    "**Uitvoeren van het Genetisch Algoritme**:\n",
    "- De functie `genetic_algorithm` wordt opgeroepen met twee parameters: `population_size` en `generations`. Hier worden respectievelijk de grootte van de populatie en het aantal generaties voor het genetisch algoritme gespecificeerd.\n",
    "- Het genetisch algoritme zal meerdere generaties doorlopen en proberen de optimale hyperparameters te vinden op basis van de prestaties van individuen.\n",
    "\n",
    "**Afdrukken van de Optimale Hyperparameters**:\n",
    "- Nadat het genetisch algoritme is voltooid, worden de optimale hyperparameters verkregen en opgeslagen in de variabele `best_hyperparameters`.\n",
    "- Deze optimale hyperparameters worden afgedrukt naar de console met de tekst \"Optimale hyperparameters:\", gevolgd door de daadwerkelijke hyperparameters.\n",
    "\n",
    "Deze code is de laatste stap in het proces van hyperparameteroptimalisatie voor regressie en onthult de optimale configuratie die is gevonden door het genetisch algoritme. Dit is belangrijke informatie om de prestaties van het neurale netwerkmodel voor regressie te verbeteren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+------------------------+\n",
      "|    Regressie Metrieken    |        Waarden         |\n",
      "+---------------------------+------------------------+\n",
      "| Mean Absolute Error (MAE) |  0.008724607985797736  |\n",
      "| Mean Squared Error (MSE)  | 0.00016080226541444667 |\n",
      "|      R-squared (R^2)      |   0.9997699172672925   |\n",
      "+---------------------------+------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Execute regressiemetrics\n",
    "best_activation_function, best_hidden_layer_size, best_solver = best_hyperparameters\n",
    "best_reg = MLPRegressor(hidden_layer_sizes=(best_hidden_layer_size,), activation=best_activation_function, solver=best_solver, max_iter=2000, random_state=42)\n",
    "best_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_reg.predict(X_test)\n",
    "\n",
    "# Calculate the regressiemetrics based on the predictions\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Make a tabulate list of the regressionmetrics and their values\n",
    "regression_metrics_values = [\n",
    "    [\"Mean Absolute Error (MAE)\", mae],\n",
    "    [\"Mean Squared Error (MSE)\", mse],\n",
    "    [\"R-squared (R^2)\", r2],\n",
    "]\n",
    "\n",
    "# Printing the tabel with regression metrics\n",
    "print(tabulate(regression_metrics_values, headers=[\"Regressie Metrieken\", \"Waarden\"], tablefmt=\"pretty\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
