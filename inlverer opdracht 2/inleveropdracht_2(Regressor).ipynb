{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetisch algioritme voor optimale hyper-parameters verkrijgen uit de iris dataset\n",
    "\n",
    "Deze code-cel begint met het importeren van de benodigde Python-bibliotheken en instellingen voor het uitvoeren van hyperparameteroptimalisatie voor een neuraal netwerkmodel. Hier zijn de belangrijkste elementen die in deze cel gebeuren:\n",
    "\n",
    "1. **Importeren van Numpy en scikit-learn Libraries**:\n",
    "   - De code importeert `numpy` als `np` voor numerieke berekeningen.\n",
    "   - Het laadt de Iris-dataset met behulp van de `load_iris` functie van scikit-learn. Deze dataset zal worden gebruikt voor het evalueren van het neuraal netwerkmodel.\n",
    "   - Het importeert `train_test_split` om de dataset te splitsen in trainings- en testgegevens.\n",
    "   - Het gebruikt `MLPRegressor` van scikit-learn om een meerlaagse perceptron-neuraal netwerk voor regressie te definiëren en te configureren.\n",
    "\n",
    "2. **Importeren van de Tabulate-bibliotheek**:\n",
    "   - De code importeert de `tabulate`-bibliotheek. Deze bibliotheek wordt later gebruikt om de resulterende error metrics op een nette manier als een tabel weer te geven.\n",
    "\n",
    "3. **Importeren van Regressiemetrieken uit sklearn**\n",
    "   - De code importeert de `mean_absolute_error`, `mean_squared_error` en `r2_score` scoringsmetrieken om het genetisch regressie algoritme te scoren.\n",
    "\n",
    "4. **Uitschakelen van Waarschuwingen**:\n",
    "   - Er wordt een waarschuwingssuppressie geactiveerd om te voorkomen dat waarschuwingen de uitvoer onnodig vervuilen.\n",
    "\n",
    "Deze voorbereidende stappen zijn cruciaal voor het correct uitvoeren van de hyperparameteroptimalisatie voor het neurale netwerkmodel en voor het weergeven van de resultaten in een overzichtelijke tabel met error metrics aan het einde van het script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports for the hyperparameter optimization\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Import the tabulate library for a nice table at the end of the file for the error metrics\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Importeer relevante regressiemetrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Ignoring all warnings because it clutters the space where the results are supposed to go in to\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gebruik van de Iris Dataset en Dataverdeling\n",
    "\n",
    "In deze code-cel wordt de Iris-dataset gebruikt en wordt de dataset verdeeld in trainings- en testgegevens. Hier zijn de belangrijkste elementen die in deze cel gebeuren:\n",
    "\n",
    "1. **Laden van de Iris Dataset**:\n",
    "   - De code laadt de Iris-dataset met behulp van de `load_iris` functie. De Iris-dataset bevat informatie over irisbloemen en wordt vaak gebruikt voor machinaal leren toepassingen.\n",
    "\n",
    "2. **Dataverdeling**:\n",
    "   - De code splitst de gegevens in onafhankelijke variabelen (`X`) en de doelvariabele (`y`). `X` bevat de kenmerken van de bloemen, terwijl `y` de bijbehorende klasselabels bevat.\n",
    "   - De dataset wordt verder opgesplitst in trainings- en testgegevens met behulp van `train_test_split`. Hierbij wordt 80% van de gegevens toegewezen aan de trainingsset en 20% aan de testset.\n",
    "   - De `random_state` wordt ingesteld op 42 (“the Ultimate Question of Life, the Universe, and Everything.\") om ervoor te zorgen dat de verdeling van de gegevens reproduceerbaar is.\n",
    "\n",
    "Deze stappen zijn essentieel voor het voorbereiden van de gegevens voor de training en evaluatie van het neurale netwerkmodel. De Iris-dataset wordt gebruikt als de basis voor dit machine learning-assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're still using the iris dataset for this assignment\n",
    "data = load_iris()\n",
    "\n",
    "# Splitting the data into training and testing data\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitie van te Testen Hyperparameters\n",
    "\n",
    "In deze code-cel worden de hyperparameters gedefinieerd die zullen worden getest bij het optimaliseren van het neurale netwerkmodel. Hier zijn de belangrijkste elementen die in deze cel gebeuren:\n",
    "\n",
    "1. **Activatiefuncties**:\n",
    "   - De code definieert een lijst `activation_functions` met verschillende activatiefuncties die zullen worden getest. Deze functies beïnvloeden de overgang van informatie tussen neuronen in het netwerk. De beschikbare activatiefuncties zijn 'logistic', 'tanh', 'relu', en 'identity'.\n",
    "\n",
    "2. **Grootte van Verborgen Lagen**:\n",
    "   - De grootte van de verborgen lagen wordt gedefinieerd als een lijst van getallen in `hidden_layer_sizes`. Elke waarde in de lijst vertegenwoordigt de grootte van een verborgen laag in het neurale netwerk. Dit geeft flexibiliteit bij het testen van verschillende architecturen.\n",
    "\n",
    "3. **Optimalisatie Solvers**:\n",
    "   - De code definieert een lijst `solvers` met verschillende optimalisatiesolvers die worden gebruikt om de optimale gewichten voor het neurale netwerk te vinden. De beschikbare solvers zijn 'lbfgs', 'sgd', en 'adam'. Het standaard leerpercentage wordt gebruikt voor elk van deze solvers.\n",
    "\n",
    "Deze definitie van hyperparameters is cruciaal voor het begeleiden van de hyperparameteroptimalisatie van het neurale netwerkmodel. Door verschillende combinaties van activatiefuncties, verborgen laaggroottes en solvers te testen, kan de optimale configuratie van het model worden bepaald.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the hyperparameters that we want to test\n",
    "activation_functions = ['logistic', 'tanh', 'relu', 'identity']\n",
    "# The hidden layer sizes are defined as a list of tuples, where each tuple represents a hidden layer\n",
    "hidden_layer_sizes = [10, 20, 30, 40]\n",
    "# The solvers are the optimizers that are used to find the optimal weights\n",
    "# We're using the default learning rate for each solver\n",
    "solvers = ['lbfgs', 'sgd', 'adam']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluatie van Individuen en Genetisch Algoritme\n",
    "\n",
    "Deze code-cel bevat de implementatie van een functie voor het evalueren van de nauwkeurigheid van individuele neurale netwerkconfiguraties, evenals een genetisch algoritme voor het zoeken naar de beste individuele configuratie. Hier zijn de belangrijkste elementen die in deze cel gebeuren:\n",
    "\n",
    "**Functie voor Evaluatie van Individuen**:\n",
    "- De functie `evaluate_individual` ontvangt drie parameters: `activation_function`, `hidden_layer_size`, en `solver`.\n",
    "- Binnen deze functie wordt een `MLPRegressor`-model geïnitialiseerd en getraind met de opgegeven hyperparameters. Het model maakt gebruik van een enkele verborgen laag met de specificaties.\n",
    "- De nauwkeurigheid van het model wordt berekend op basis van de testgegevens (`X_test` en `y_test`) en geretourneerd als het evaluatieresultaat.\n",
    "\n",
    "**Genetisch Algoritme voor Hyperparameteroptimalisatie**:\n",
    "- De functie `genetic_algorithm` wordt gebruikt om het genetisch algoritme uit te voeren voor hyperparameteroptimalisatie.\n",
    "- Een initiële populatie van individuen wordt gecreëerd, waarbij de hyperparameters willekeurig worden gekozen uit de gedefinieerde sets van activatiefuncties, verborgen laaggroottes en solvers.\n",
    "- Het genetisch algoritme wordt vervolgens uitgevoerd over meerdere generaties, waarbij individuen worden geëvalueerd en de beste individuen worden geselecteerd op basis van hun nauwkeurigheid.\n",
    "- Nieuwe individuen worden gegenereerd door combinaties van ouders en een willekeurige mutatie wordt geïntroduceerd om diversiteit te behouden.\n",
    "- Het genetisch algoritme zoekt naar het beste individu met de hoogste nauwkeurigheid en retourneert de hyperparameters van dat individu als resultaat.\n",
    "\n",
    "Deze implementaties zijn cruciaal voor het automatisch zoeken naar de optimale hyperparameters van het neurale netwerkmodel. Het genetisch algoritme evolueert de populatie van individuen om de configuratie te vinden die leidt tot de beste nauwkeurigheid bij regressie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function that evaluates the accuracy of an individual\n",
    "def evaluate_individual(activation_function, hidden_layer_size, solver):\n",
    "    clf = MLPRegressor(hidden_layer_sizes=(hidden_layer_size,), activation=activation_function, solver=solver, max_iter=2000, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    \n",
    "\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# The genetic algorithm that finds the best individual\n",
    "def genetic_algorithm(population_size, generations):\n",
    "    population = []\n",
    "\n",
    "    # Creating the initial population\n",
    "    for _ in range(population_size):\n",
    "        activation_function = np.random.choice(activation_functions)\n",
    "        hidden_layer_size = np.random.choice(hidden_layer_sizes)\n",
    "        solver = np.random.choice(solvers)\n",
    "        individual = (activation_function, hidden_layer_size, solver)\n",
    "        population.append(individual)\n",
    "\n",
    "    # Running the genetic algorithm\n",
    "    for generation in range(generations):\n",
    "        scores = [evaluate_individual(activation_function, hidden_layer_size, solver) for activation_function, hidden_layer_size, solver in population]\n",
    "        selected_indices = np.argsort(scores)[::-1][:int(population_size * 0.2)]\n",
    "        selected_population = [population[i] for i in selected_indices]\n",
    "\n",
    "        # Creating the new population\n",
    "        new_population = []\n",
    "        for _ in range(population_size):\n",
    "            index1, index2 = np.random.choice(len(selected_population), size=2, replace=False)\n",
    "            parent1 = selected_population[index1]\n",
    "            parent2 = selected_population[index2]\n",
    "            parent1_activation_function, parent1_hidden_layer_size, parent1_solver = parent1\n",
    "            parent2_activation_function, parent2_hidden_layer_size, parent2_solver = parent2\n",
    "            child = (parent1_activation_function, parent2_hidden_layer_size, parent2_solver)\n",
    "            new_population.append(child)\n",
    "\n",
    "        # Mutating the new population\n",
    "        population = new_population\n",
    "\n",
    "    # Finding the best individual\n",
    "    best_individual = max(population, key=lambda ind: evaluate_individual(ind[0], ind[1], ind[2]))\n",
    "    return best_individual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uitvoeren van het Genetisch Algoritme en Afdrukken van Resultaten\n",
    "\n",
    "In deze code-cel wordt het genetisch algoritme uitgevoerd om de optimale hyperparameters voor het neurale netwerkmodel te vinden, en worden de resultaten afgedrukt. Hier zijn de belangrijkste elementen die in deze cel gebeuren:\n",
    "\n",
    "**Uitvoeren van het Genetisch Algoritme**:\n",
    "- De functie `genetic_algorithm` wordt opgeroepen met twee parameters: `population_size` en `generations`. Hier worden respectievelijk de grootte van de populatie en het aantal generaties voor het genetisch algoritme gespecificeerd.\n",
    "- Het genetisch algoritme zal meerdere generaties doorlopen en proberen de optimale hyperparameters te vinden op basis van de prestaties van individuen.\n",
    "\n",
    "**Afdrukken van de Optimale Hyperparameters**:\n",
    "- Nadat het genetisch algoritme is voltooid, worden de optimale hyperparameters verkregen en opgeslagen in de variabele `best_hyperparameters`.\n",
    "- Deze optimale hyperparameters worden afgedrukt naar de console met de tekst \"Optimal hyperparameters:\", gevolgd door de daadwerkelijke hyperparameters.\n",
    "\n",
    "Deze code is de laatste stap in het proces van hyperparameteroptimalisatie en onthult de optimale configuratie die is gevonden door het genetisch algoritme. Dit is belangrijke informatie om de prestaties van het neurale netwerkmodel te verbeteren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal hyperparameters: ('tanh', 10, 'lbfgs')\n"
     ]
    }
   ],
   "source": [
    "# Running the genetic algorithm and printing the results\n",
    "best_hyperparameters = genetic_algorithm(population_size=50, generations=10)\n",
    "print(\"Optimal hyperparameters:\", best_hyperparameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uitvoeren van het Genetisch Algoritme voor Regressie en de Evaluatie van het Geoptimaliseerde Neurale Netwerk Model Algoritme (met de juiste error metrics)\n",
    "\n",
    "In deze code-cel wordt het genetisch algoritme voor regressie uitgevoerd om de optimale hyperparameters voor het neurale netwerkmodel te vinden, en worden de resultaten afgedrukt. Hier zijn de belangrijkste elementen die in deze cel gebeuren:\n",
    "\n",
    "**Uitvoeren van het Genetisch Algoritme**:\n",
    "- De functie `genetic_algorithm` wordt opgeroepen met twee parameters: `population_size` en `generations`. Hier worden respectievelijk de grootte van de populatie en het aantal generaties voor het genetisch algoritme gespecificeerd.\n",
    "- Het genetisch algoritme zal meerdere generaties doorlopen en proberen de optimale hyperparameters te vinden op basis van de prestaties van individuen.\n",
    "\n",
    "**Afdrukken van de Optimale Hyperparameters**:\n",
    "- Nadat het genetisch algoritme is voltooid, worden de optimale hyperparameters verkregen en opgeslagen in de variabele `best_hyperparameters`.\n",
    "- Deze optimale hyperparameters worden afgedrukt naar de console met de tekst \"Optimale hyperparameters:\", gevolgd door de daadwerkelijke hyperparameters.\n",
    "\n",
    "Deze code is de laatste stap in het proces van hyperparameteroptimalisatie voor regressie en onthult de optimale configuratie die is gevonden door het genetisch algoritme. Dit is belangrijke informatie om de prestaties van het neurale netwerkmodel voor regressie te verbeteren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+------------------------+\n",
      "|    Regressie Metrieken    |        Waarden         |\n",
      "+---------------------------+------------------------+\n",
      "| Mean Absolute Error (MAE) |  0.008724607985797736  |\n",
      "| Mean Squared Error (MSE)  | 0.00016080226541444667 |\n",
      "|      R-squared (R^2)      |   0.9997699172672925   |\n",
      "+---------------------------+------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Execute regressiemetrics\n",
    "best_activation_function, best_hidden_layer_size, best_solver = best_hyperparameters\n",
    "best_clf = MLPRegressor(hidden_layer_sizes=(best_hidden_layer_size,), activation=best_activation_function, solver=best_solver, max_iter=2000, random_state=42)\n",
    "best_clf.fit(X_train, y_train)\n",
    "y_pred = best_clf.predict(X_test)\n",
    "\n",
    "# Calculate the regressiemetrics based on the predictions\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Make a tabulate list of the regressionmetrics and their values\n",
    "regression_metrics_values = [\n",
    "    [\"Mean Absolute Error (MAE)\", mae],\n",
    "    [\"Mean Squared Error (MSE)\", mse],\n",
    "    [\"R-squared (R^2)\", r2],\n",
    "]\n",
    "\n",
    "# Printing the tabel with regression metrics\n",
    "print(tabulate(regression_metrics_values, headers=[\"Regressie Metrieken\", \"Waarden\"], tablefmt=\"pretty\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
